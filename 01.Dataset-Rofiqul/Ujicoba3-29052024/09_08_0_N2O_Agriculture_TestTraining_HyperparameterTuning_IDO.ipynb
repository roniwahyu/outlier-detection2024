{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to google drive folder\n",
    "# %cd /content/drive/MyDrive/Colab Notebooks/Disertasi-Ahmad-Rofiqul/002. Laporan-eksperimen\n",
    "# %cd /content/drive/MyDrive/Colab Notebooks/Disertasi-Ahmad-Rofiqul/002.Experiment_SWI_16052024/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor, DMatrix, cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.ensemble import IsolationForest, VotingClassifier\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# from sklearn.svm import OneClassSVM\n",
    "# from sklearn.covariance import EllipticEnvelope\n",
    "# from pyod.models.hbos import HBOS\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, roc_auc_score, roc_curve, auc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.base import BaseEstimator, OutlierMixin\n",
    "# from scipy.spatial.distance import mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "# df = pd.read_csv('/mnt/data/09_06_0_N2O_Agriculture_TestTraining_IDO_CV.ipynb')\n",
    "file_path = 'dataset/09_05_4_1_AgricultureOutliers_HardVotingBased_IDO.csv'\n",
    "df= pd.read_csv(file_path)\n",
    "\n",
    "X = df.drop('N2O', axis=1)\n",
    "y = df['N2O']\n",
    "\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.35]\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# Function to perform no cross-validation\n",
    "def no_cross_validation(X_train, y_train, X_test, y_test, params):\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return evaluate_model(y_test, y_pred)\n",
    "\n",
    "# Function to perform sklearn cross_val_score\n",
    "def sklearn_cross_val_score_eval(X, y, params):\n",
    "    model = XGBRegressor(**params)\n",
    "    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(mse_scores)\n",
    "    r2_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    return mae_scores.mean(), mse_scores.mean(), rmse_scores.mean(), r2_scores.mean()\n",
    "\n",
    "# Function to perform xgb.cv\n",
    "def xgb_cv_eval(X, y, params):\n",
    "    dtrain = DMatrix(X, label=y)\n",
    "    cv_results = cv(params, dtrain, num_boost_round=100, nfold=5, metrics=['mae', 'rmse'], early_stopping_rounds=10, seed=42)\n",
    "    mae = cv_results['test-mae-mean'].iloc[-1]\n",
    "    rmse = cv_results['test-rmse-mean'].iloc[-1]\n",
    "    mse = rmse ** 2\n",
    "    r2 = 1 - (mse / np.var(y))\n",
    "    return mae, mse, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform hyperparameter tuning and evaluation\n",
    "def perform_gridsearch(X, y, test_size):\n",
    "    results = {'Test Size': [], 'Method': [], 'MAE': [], 'MSE': [], 'RMSE': [], 'R2': []}\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Grid Search\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=XGBRegressor(n_estimators=100, random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # No Cross Validation\n",
    "    mae, mse, rmse, r2 = no_cross_validation(X_train, y_train, X_test, y_test, best_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('No Cross Validation (Grid Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "\n",
    "    # Sklearn cross_val_score\n",
    "    mae, mse, rmse, r2 = sklearn_cross_val_score_eval(X, y, best_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('Sklearn cross_val_score (Grid Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "    \n",
    "    # xgb.cv\n",
    "    xgb_params = best_params.copy()\n",
    "    xgb_params['objective'] = 'reg:squarederror'\n",
    "    mae, mse, rmse, r2 = xgb_cv_eval(X, y, xgb_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('xgb.cv (Grid Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform hyperparameter tuning and evaluation\n",
    "def perform_randomsearch(X, y, test_size):\n",
    "    results = {'Test Size': [], 'Method': [], 'MAE': [], 'MSE': [], 'RMSE': [], 'R2': []}\n",
    "    \n",
    "    # Random Search\n",
    "    param_distributions = {\n",
    "        'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "        'max_depth': [3, 5, 7, 9, 11],\n",
    "        'min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=XGBRegressor(n_estimators=100, random_state=42),\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=50,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_params = random_search.best_params_\n",
    "    \n",
    "    # No Cross Validation\n",
    "    mae, mse, rmse, r2 = no_cross_validation(X_train, y_train, X_test, y_test, best_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('No Cross Validation (Random Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "\n",
    "    # Sklearn cross_val_score\n",
    "    mae, mse, rmse, r2 = sklearn_cross_val_score_eval(X, y, best_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('Sklearn cross_val_score (Random Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "    \n",
    "    # xgb.cv\n",
    "    xgb_params = best_params.copy()\n",
    "    xgb_params['objective'] = 'reg:squarederror'\n",
    "    mae, mse, rmse, r2 = xgb_cv_eval(X, y, xgb_params)\n",
    "    results['Test Size'].append(test_size)\n",
    "    results['Method'].append('xgb.cv (Random Search)')\n",
    "    results['MAE'].append(mae)\n",
    "    results['MSE'].append(mse)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R2'].append(r2)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 729 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n729 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1055, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 521, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 958, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 1529, in __init__\n    self._init(\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 1588, in _init\n    it.reraise()\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 576, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 557, in _handle_exception\n    return fn()\n           ^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 641, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n                                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 1280, in next\n    input_data(**self.kwargs)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 624, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 1315, in _proxy_transform\n    arr, feature_names, feature_types = _transform_pandas_df(\n                                        ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 490, in _transform_pandas_df\n    _invalid_dataframe_dtype(data)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 308, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Date: object, Experiment: object, DataUse: object, Replication: object, Month: object, Vegetation: object, VegType: object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_size \u001b[38;5;129;01min\u001b[39;00m test_sizes:\n\u001b[1;32m----> 5\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_gridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# results2 = perform_randomsearch(X, y, test_size)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(results))\n",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m, in \u001b[0;36mperform_gridsearch\u001b[1;34m(X, y, test_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m]\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     16\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     17\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mXGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     18\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# No Cross Validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:945\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    943\u001b[0m     )\n\u001b[1;32m--> 945\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 729 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n729 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1055, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 521, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 958, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 1529, in __init__\n    self._init(\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 1588, in _init\n    it.reraise()\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 576, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 557, in _handle_exception\n    return fn()\n           ^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 641, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n                                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 1280, in next\n    input_data(**self.kwargs)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 624, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 1315, in _proxy_transform\n    arr, feature_names, feature_types = _transform_pandas_df(\n                                        ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 490, in _transform_pandas_df\n    _invalid_dataframe_dtype(data)\n  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\data.py\", line 308, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Date: object, Experiment: object, DataUse: object, Replication: object, Month: object, Vegetation: object, VegType: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform testing for each test size and optimization method\n",
    "all_results = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    results = perform_gridsearch(X, y, test_size)\n",
    "    # results2 = perform_randomsearch(X, y, test_size)\n",
    "    all_results.append(pd.DataFrame(results))\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the DataFrame\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Hyperparameter Tuning Results\", dataframe=final_results_df)\n",
    "\n",
    "# Visualization of performance metrics\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n",
    "methods = final_results_df['Method'].unique()\n",
    "colors = ['b', 'g', 'r', 'c', 'm']  # Adjust or expand this list as needed\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, method in enumerate(methods):\n",
    "        subset = final_results_df[final_results_df['Method'] == method]\n",
    "        plt.bar(subset['Test Size'] + i * 0.02, subset[metric], width=0.02, label=method, color=colors[i % len(colors)])\n",
    "        \n",
    "        for x, y in zip(subset['Test Size'], subset[metric]):\n",
    "            plt.text(x + i * 0.02, y, f'{y:.5f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.xlabel('Test Size')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Comparison of {metric} for Different Hyperparameter Tuning Methods')\n",
    "    plt.xticks(test_sizes)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
