{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths and their corresponding methods\n",
    "file_paths = {\n",
    "    'randomsearch': 'hasil/uji4/combined_xgboost_evaluations_with_group_randomsearch.csv',\n",
    "    'gridsearch': 'hasil/uji4/combined_xgboost_evaluations_with_group_gridsearch.csv',\n",
    "    'notuning': 'hasil/uji4/combined_xgboost_evaluations_with_group_notuning.csv'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best overall method and evaluation based on highest score:\n",
      "Test Size                                                        0.2\n",
      "Method                           No Cross Validation (Random Search)\n",
      "Best Params        {'subsample': 0.8, 'min_child_weight': 2, 'max...\n",
      "Start Time                                       2024-06-10 22:49:52\n",
      "End Time                                         2024-06-10 22:50:09\n",
      "Duration (s)                                                17.56033\n",
      "MAE                                                         2.262814\n",
      "MSE                                                        13.040409\n",
      "RMSE                                                        3.611151\n",
      "R2                                                          0.799246\n",
      "Group                                      CV_RandomSearch_if_by_lof\n",
      "MethodGroup                                             randomsearch\n",
      "Normalized_R2                                                    1.0\n",
      "Normalized_MAE                                              0.028064\n",
      "Normalized_MSE                                               0.05082\n",
      "Normalized_RMSE                                             0.025741\n",
      "Score                                                       0.421157\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Best overall method and evaluation based on combined weighted criteria:\n",
      "Test Size                                                        0.2\n",
      "Method                           No Cross Validation (Random Search)\n",
      "Best Params        {'subsample': 0.8, 'min_child_weight': 2, 'max...\n",
      "Start Time                                       2024-06-10 22:49:52\n",
      "End Time                                         2024-06-10 22:50:09\n",
      "Duration (s)                                                17.56033\n",
      "MAE                                                         2.262814\n",
      "MSE                                                        13.040409\n",
      "RMSE                                                        3.611151\n",
      "R2                                                          0.799246\n",
      "Group                                      CV_RandomSearch_if_by_lof\n",
      "MethodGroup                                             randomsearch\n",
      "Normalized_R2                                                    1.0\n",
      "Normalized_MAE                                              0.028064\n",
      "Normalized_MSE                                               0.05082\n",
      "Normalized_RMSE                                             0.025741\n",
      "Score                                                       0.421157\n",
      "Name: 7, dtype: object\n",
      "Summary of best evaluations saved as hasil/uji4/0909_1_2_improved_best_evaluations_summary_weighted_scoring.csv\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the summary data\n",
    "summary_data = {}\n",
    "\n",
    "# Define the evaluation metrics\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'R2']\n",
    "\n",
    "# Load and process each file\n",
    "for method_group, file_path in file_paths.items():\n",
    "    try:\n",
    "        # Load data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create a dictionary to store the best evaluations for each metric\n",
    "        best_evaluations = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in data.columns:\n",
    "                # Find the row with the best value (min for errors, max for R2)\n",
    "                if metric == 'R2':\n",
    "                    best_evaluations[metric] = data.loc[data.groupby('Group')[metric].idxmax()]\n",
    "                else:\n",
    "                    best_evaluations[metric] = data.loc[data.groupby('Group')[metric].idxmin()]\n",
    "        \n",
    "        # Combine the best evaluations into a single DataFrame\n",
    "        best_evaluations_df = pd.concat(best_evaluations.values()).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        # Add the method group to the DataFrame\n",
    "        best_evaluations_df['MethodGroup'] = method_group\n",
    "        \n",
    "        # Store the best evaluations in the summary data\n",
    "        summary_data[method_group] = best_evaluations_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Combine all summaries into a single DataFrame\n",
    "summary_df = pd.concat(summary_data.values(), ignore_index=True)\n",
    "\n",
    "# Normalize the metrics to a 0-1 scale\n",
    "summary_df['Normalized_R2'] = summary_df['R2'] / summary_df['R2'].max()\n",
    "summary_df['Normalized_MAE'] = 1 - (summary_df['MAE'] / summary_df['MAE'].max())\n",
    "summary_df['Normalized_MSE'] = 1 - (summary_df['MSE'] / summary_df['MSE'].max())\n",
    "summary_df['Normalized_RMSE'] = 1 - (summary_df['RMSE'] / summary_df['RMSE'].max())\n",
    "\n",
    "# Define weights for each metric\n",
    "weight_R2 = 0.4\n",
    "weight_MAE = 0.3\n",
    "weight_MSE = 0.2\n",
    "weight_RMSE = 0.1\n",
    "\n",
    "# Calculate the weighted score for each method\n",
    "summary_df['Score'] = (\n",
    "    (weight_R2 * summary_df['Normalized_R2']) +\n",
    "    (weight_MAE * summary_df['Normalized_MAE']) +\n",
    "    (weight_MSE * summary_df['Normalized_MSE']) +\n",
    "    (weight_RMSE * summary_df['Normalized_RMSE'])\n",
    ")\n",
    "sorted_summary_df = summary_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "# Print the top result for quick reference\n",
    "print(\"\\nBest overall method and evaluation based on highest score:\")\n",
    "print(sorted_summary_df.iloc[0])\n",
    "\n",
    "# Save the sorted summarized results to a CSV file\n",
    "sorted_summary_file_path = 'hasil/uji4/0909_1_1_improved_best_evaluations_summary_weighted_scoring_sorted.csv'\n",
    "sorted_summary_df.to_csv(sorted_summary_file_path, index=False)\n",
    "\n",
    "# Identify the best method based on the highest score\n",
    "best_overall = summary_df.loc[summary_df['Score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest overall method and evaluation based on combined weighted criteria:\")\n",
    "print(best_overall)\n",
    "\n",
    "# Save the summarized results to a CSV file\n",
    "summary_file_path = 'hasil/uji4/0909_1_2_improved_best_evaluations_summary_weighted_scoring.csv'\n",
    "summary_df.to_csv(summary_file_path, index=False)\n",
    "\n",
    "print(f\"Summary of best evaluations saved as {summary_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
